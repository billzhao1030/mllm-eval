# General Experiment Configuration
experiment:
  id: "default"                # Experiment ID
  output_dir: "output"         # Directory for output/experiment ID
  data_dir: "../data"          # Root directory for the datasets
  seed: 0                      # Random seed for reproducibility
  max_saved_ckpts: 1           # Maximum number of saved checkpoints
  eval_first: false            # Evaluate before training
  debug: false                 # Debug mode
  test: false                  # Test mode
  submit: false                # Submission mode
  no_backtrack: false          # Disable backtracking
  detailed_output: false       # Enable detailed output
  model: None
  max_step: 30

  # Checkpoint Configuration
  resume_file: null            # Path to resume a trained model
  resume_optimizer: true       # Whether to resume optimizer state
  save_latest_states: true     # Save the latest states

  # Logging with Neptune.ai
  use_neptune: false           # Use Neptune.ai for logging
  neptune_project: null        # Neptune project name
  neptune_api_token: null      # Neptune API token
  tags: []                     # Tags for the experiment
  batch_size: 1                # Batch size
  workers: 1                   # Number of workers

# Agent Configuration
agent:
  type: "duet"                 # Agent type
  ignoreid: -100               # ID to ignore certain actions
  enc_full_graph: true         # Encode full graph
  act_visited_nodes: false     # Whether to act on visited nodes
  fusion: "dynamic"            # Fusion method: global, local, avg, dynamic
  expl_sample: false           # Exploration sampling
  expl_max_ratio: 0.6          # Maximum exploration ratio
  expert_policy: "spl"         # Expert policy choice: spl, ndtw

# MP3D Environment
environment:
  obs_dir: "../data/marked_obs/"
  caption_dir: "../data/MP3D/GPT-4o_caption.json"
  obs_sum_dir: null
  map_dir: null

# Model Configuration
model:
  fix_lang_embedding: false    # Freeze language embeddings
  fix_pano_embedding: false    # Freeze panorama embeddings
  fix_local_branch: false      # Freeze local branch
  num_l_layers: 9              # Number of language layers
  num_pano_layers: 2           # Number of panorama layers
  num_x_layers: 4              # Number of cross-attention layers
  graph_sprels: true           # Use spatial relations in the graph
  load_text_features: false    # Load precomputed text features
  text_feat_size: 2048         # Size of text features
  gmap_max_action_steps: 100   # Maximum number of action steps in the global map
  max_instr_len: 80            # Maximum instruction length
  max_objects: 70              # Maximum number of objects
  pretrained_ckpt:             # Path to pretrained checkpoint
    "../data/pretrain/Attnkv_pretrained_ckpt.pt"

  # Mixture of Experts Configuration
  use_moe_layer: true          # Use mixture of experts layer
  moe_type: "Task"             # MoE type: Task or Sparse
  moe_position: "Attn_kv"      # MoE position: Attn_kv, Attn_q or FFN
  task_routing_feature: "cls"  # Task routing feature: cls, mean, task_id, task_id_cls, task_id_multi or multi
  num_experts: 8               # Number of experts
  num_experts_per_tok: 2       # Number of experts per token
  router_aux_loss_coef: 0.8    # Router auxiliary loss coefficient

  # Dropout Parameters
  hidden_dropout_prob: 0.1     # Dropout rate
  attn_dropout_prob: 0.1       # Feature dropout rate
  feat_dropout: 0.3            # Feature dropout rate


# Distributed Training Configuration
distributed:
  dist_backend: "nccl"         # Backend choice: nccl, gloo
  init_method: "env://"        # Initialization method
  world_size: 1                # Number of processes
  rank: 0                      # Rank of the current process
  local_rank: -1               # Local rank of the current process
  no_set_device_rank: false    # Do not set device rank


# Tasks Configuration
task:
  val_source:                  # Source datasets for testing
    ['RXR', 'R2R', 'REVERIE', 'CVDN']
  ratio:                       # Training ratio of each dataset
    [20, 1, 1, 10, 1, 1, 1, 1, 2]
  loss_coef: {                 # Loss coefficient for each dataset
    "R2R": 1.0,
    "REVERIE": 1.0,
    "RXR_EN": 1.0,
    "CVDN": 1.0,
  }
  val_max_action_len: {        # Maximum action length for validation
    "R2R": 15,
    "REVERIE": 15,
    "RXR_EN": 30,
    "CVDN": 30,                # from VLN-SIG
  }
  train_max_action_len: {      # Maximum action length for training
    "R2R": 15,
    "REVERIE": 15,
    "RXR_EN": 20,
    "CVDN": 15,
  }

  eval_splits: {                # Evaluation splits
    "R2R": ["val_unseen_subset"],
    "REVERIE": ["val_unseen_subset"],
    "RXR": ["val_unseen_subset", "val_unseen"],
    "CVDN": ["val_unseen_subset", "val_unseen"],
  }

# Datasets Configuration
dataset:
  R2R:                          # Room-to-Room Navigation
    DIR: "R2R"                  # Dataset directory
    SPLIT: {                    # Split files
      "train": "R2R_train_mergesim_enc.json",
      "val_train_seen": "R2R_val_train_seen_enc.json",
      "val_seen": "R2R_val_seen_enc.json",
      "val_unseen": "R2R_val_unseen_enc.json",
      "test": "R2R_test_enc.json"
    }
  REVERIE:                      # REVERIE
    DIR: "REVERIE"              # Dataset directory
    bbox_file: "BBoxes.json"    # Bounding box file
    multi_endpoints: true       # Use multiple endpoints
    SPLIT: {                    # Split files
      "train": "REVERIE_train_enc.json",
      "val_train_seen": "REVERIE_val_train_seen_enc.json",
      "val_seen": "REVERIE_val_seen_enc.json",
      "val_unseen": "REVERIE_val_unseen_enc.json",
      "test": "REVERIE_test_enc.json"
    }
  RXR_EN:                       # RXR-EN
    DIR: "RXR_EN"               # Dataset directory
    SPLIT: {                    # Split files
      "train": "RXR_EN_train_enc.json",
      "val_seen": "RXR_EN_val_seen_enc.json",
      "val_unseen": "RXR_EN_val_unseen_enc.json",
    }
  CVDN:                         # CVDN
    DIR: "CVDN"                 # Dataset directory
    path_type: "trusted_path"   # Path type: trusted_path, planner_path
    SPLIT: {                    # Split files
      "train": "train.json",
      "val_seen": "val_seen.json",
      "val_unseen": "val_unseen.json",
      "test": "test_cleaned.json"
    }